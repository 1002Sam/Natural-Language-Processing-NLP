{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJNcLQMXbVJf6dH6FQH/0V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1002Sam/Natural-Language-Processing-NLP/blob/main/Natural_Language_Processing_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My Self Learning of NLP"
      ],
      "metadata": {
        "id": "b7uqDPXgYrqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Gz_7wzgsTXs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6983a0ad-7115-476c-8f89-0f4c4ac542ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "69T47Xyunb0t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n"
      ],
      "metadata": {
        "id": "dNTenHI9reyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing sentences : convert paragraph into list of sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "mJYMWJVBYMO5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B658T5kAxdIb",
        "outputId": "8c8dc16c-a618-463d-ef75-76ae9a756b17"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over\\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture,\\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my\\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of\\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India’s development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India\\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be\\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of\\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing words : convert paragraph into list of words\n",
        "words = nltk.word_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "I1DUjkvcYhE7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Siu7fpiDrv-L",
        "outputId": "98a18ba3-59cd-4fe7-d6fd-5e6ca4707cb9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "399"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Part-of-Speech(POS) Tagging"
      ],
      "metadata": {
        "id": "G1mu_uWFrtfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " POS tagging is the process of labelling a word in a text as corresponding to a particular POS tag: nouns, verbs, adjectives, adverbs, etc. The average perceptron tagger uses the perceptron algorithm to predict which POS tag is most likely given the word1. Averaged_perceptron_tagger is used in Natural Language Toolkit (NLTK). Apertag is another sequence tagger based on an averaged perceptron mode."
      ],
      "metadata": {
        "id": "e0Q93PBfrrD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')  #it can be used to tag words in a sentence with their part-of-speech (POS) categories.\n",
        "\n",
        "text = \"\"\"Do you love programming in Python?\n",
        "          Microbes are tiny creatures, which aren't visible to the eye; that's why we use \"Microscope\" for studying them.\n",
        "          Wow! She quickly gave him three red apples before the train left.\"\"\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzyLOsd9peXc",
        "outputId": "11531e31-dd4e-4790-99cf-a73a1e577254"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Do', 'VB'), ('you', 'PRP'), ('love', 'VB'), ('programming', 'VBG'), ('in', 'IN'), ('Python', 'NNP'), ('?', '.'), ('Microbes', 'NNP'), ('are', 'VBP'), ('tiny', 'JJ'), ('creatures', 'NNS'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), (\"n't\", 'RB'), ('visible', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('eye', 'NN'), (';', ':'), ('that', 'DT'), (\"'s\", 'VBZ'), ('why', 'WRB'), ('we', 'PRP'), ('use', 'VBP'), ('``', '``'), ('Microscope', 'NNP'), (\"''\", \"''\"), ('for', 'IN'), ('studying', 'VBG'), ('them', 'PRP'), ('.', '.'), ('Wow', 'NN'), ('!', '.'), ('She', 'PRP'), ('quickly', 'RB'), ('gave', 'VBD'), ('him', 'PRP'), ('three', 'CD'), ('red', 'JJ'), ('apples', 'NNS'), ('before', 'IN'), ('the', 'DT'), ('train', 'NN'), ('left', 'VBD'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tagset\n",
        "List of common POS tags along with their nomenclature in the Penn Treebank Tagset:\n",
        "\n",
        "CC - Coordinating Conjunction\n",
        "CD - Cardinal Digit\n",
        "DT - Determiner\n",
        "EX - Existential There\n",
        "FW - Foreign Word\n",
        "IN - Preposition or Subordinating Conjunction\n",
        "JJ - Adjective\n",
        "JJR - Adjective, Comparative\n",
        "JJS - Adjective, Superlative\n",
        "LS - List Item Marker\n",
        "MD - Modal\n",
        "NN - Noun, Singular or Mass\n",
        "NNS - Noun, Plural\n",
        "NNP - Proper Noun, Singular\n",
        "NNPS - Proper Noun, Plural\n",
        "PDT - Predeterminer\n",
        "POS - Possessive Ending\n",
        "PRP - Personal Pronoun\n",
        "*PRP*$ - Possessive Pronoun\n",
        "RB - Adverb\n",
        "RBR - Adverb, Comparative\n",
        "RBS - Adverb, Superlative\n",
        "RP - Particle\n",
        "SYM - Symbol\n",
        "TO - to\n",
        "UH - Interjection\n",
        "VB - Verb, Base Form\n",
        "VBD - Verb, Past Tense\n",
        "VBG - Verb, Gerund or Present Participle\n",
        "VBN - Verb, Past Participle\n",
        "VBP - Verb, Non-3rd Person Singular Present\n",
        "VBZ - Verb, 3rd Person Singular Present\n",
        "WDT - Wh-determiner\n",
        "WP - Wh-pronoun\n",
        "WP$ - Possessive Wh-pronoun\n",
        "WRB - Wh-adverb\n",
        "\n",
        "Please note that the exact set of POS tags can vary depending on the specific NLP library or tool you are using, but the Penn Treebank Tagset is one of the most widely used and recognized tagsets in NLP."
      ],
      "metadata": {
        "id": "G8EDs-GoupZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "sPRxuCO5sDkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords  #for getting a set of stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer #stemming algorithm used for word normalization #SnowballStemmer also used which is advanced than PorterStemmer & support multiple languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iAdmqByjdeG",
        "outputId": "e044572c-10f1-4b19-ae0a-f7bc295814aa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance named stemmer for class PorterStemmer()\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "Ww4J2UVMkue9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert paragraph into list of sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "cBHFtLfMtcjs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert/tokenize paragraph into list of words/tokens\n",
        "words = nltk.word_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "sMn6_9xrx4BX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBg6bmDrtk8Z",
        "outputId": "bc8f043f-b6d0-49ad-e8d7-c0d6d1ba5226"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "range(len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDBrxDfxtww1",
        "outputId": "1d23f49c-c15b-4263-d04f-f53db4a71a94"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming the sentences\n",
        "for i in range(len(sentences)):                              #range(len(sentences)) alternate for this =  range(0, len(messages))\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "2PPiq2vynX9p"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JfmrwCIn0XI",
        "outputId": "1121831c-d20b-4121-98aa-6b6a28c61041"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'we conquer anyon .',\n",
              " 'we grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'i believ india got first vision 1857 , start war independ .',\n",
              " 'it freedom must protect nurtur build .',\n",
              " 'if free , one respect us .',\n",
              " 'my second vision india ’ develop .',\n",
              " 'for fifti year develop nation .',\n",
              " 'it time see develop nation .',\n",
              " 'we among top 5 nation world term gdp .',\n",
              " 'we 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " 'isn ’ incorrect ?',\n",
              " 'i third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus i believ unless india stand world , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must strong militari power also econom power .',\n",
              " 'both must go hand-in-hand .',\n",
              " 'my good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'i lucki work three close consid great opportun life .',\n",
              " 'i see four mileston career']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print each processed sentence\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrMgexRs2XpK",
        "outputId": "708c9f3c-27a1-4d78-edd2-ba79ed4803b8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i three vision india .\n",
            "in 3000 year histori , peopl world come invad us , captur land , conquer mind .\n",
            "from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .\n",
            "yet done nation .\n",
            "we conquer anyon .\n",
            "we grab land , cultur , histori tri enforc way life .\n",
            "whi ?\n",
            "becaus respect freedom others.that first vision freedom .\n",
            "i believ india got first vision 1857 , start war independ .\n",
            "it freedom must protect nurtur build .\n",
            "if free , one respect us .\n",
            "my second vision india ’ develop .\n",
            "for fifti year develop nation .\n",
            "it time see develop nation .\n",
            "we among top 5 nation world term gdp .\n",
            "we 10 percent growth rate area .\n",
            "our poverti level fall .\n",
            "our achiev global recognis today .\n",
            "yet lack self-confid see develop nation , self-reli self-assur .\n",
            "isn ’ incorrect ?\n",
            "i third vision .\n",
            "india must stand world .\n",
            "becaus i believ unless india stand world , one respect us .\n",
            "onli strength respect strength .\n",
            "we must strong militari power also econom power .\n",
            "both must go hand-in-hand .\n",
            "my good fortun work three great mind .\n",
            "dr. vikram sarabhai dept .\n",
            "space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .\n",
            "i lucki work three close consid great opportun life .\n",
            "i see four mileston career\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for loop iteration to know the token in the list of sentences\n",
        "# for i in range(len(sentences)):\n",
        "#   words = nltk.word_tokenize(sentences[i])\n",
        "# print(words)"
      ],
      "metadata": {
        "id": "rzJOG-x_2Xln"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stemmed words after removing the stopwords contained in words\n",
        "# words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "# print(words)\n"
      ],
      "metadata": {
        "id": "genXEqn82Xi9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the stemmmed words sentences after removing stopwords into a sentence back again.\n",
        "# sentences = ' '.join(words)\n",
        "# print(sentences)"
      ],
      "metadata": {
        "id": "rZejyZTP3QMs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "DWgUK7USKj1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer   #lemmatizing algorithm used for word normalization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ7CeDTdKrsW",
        "outputId": "a2e2cff3-978f-412c-9a9b-d5b409438597"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance named stemmer for class PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "vjllAfmzLLPO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert paragraph into list of sentences\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "_tMPi2p33ym_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUpahCLhLyC3",
        "outputId": "00fd9bb6-7f3c-41a4-cb08-b46d55f16ae3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "range(len(sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sdrpdHVMPtb",
        "outputId": "8c2f1531-a3a5-4e12-d5ee-ea9bc4a9c1de"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)"
      ],
      "metadata": {
        "id": "Hp5envhn3yul"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO92Hazp2XfV",
        "outputId": "db0eebd6-0572-431c-e5f0-3589820854b3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I three vision India .',\n",
              " 'In 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
              " 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .',\n",
              " 'Yet done nation .',\n",
              " 'We conquered anyone .',\n",
              " 'We grabbed land , culture , history tried enforce way life .',\n",
              " 'Why ?',\n",
              " 'Because respect freedom others.That first vision freedom .',\n",
              " 'I believe India got first vision 1857 , started War Independence .',\n",
              " 'It freedom must protect nurture build .',\n",
              " 'If free , one respect u .',\n",
              " 'My second vision India ’ development .',\n",
              " 'For fifty year developing nation .',\n",
              " 'It time see developed nation .',\n",
              " 'We among top 5 nation world term GDP .',\n",
              " 'We 10 percent growth rate area .',\n",
              " 'Our poverty level falling .',\n",
              " 'Our achievement globally recognised today .',\n",
              " 'Yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
              " 'Isn ’ incorrect ?',\n",
              " 'I third vision .',\n",
              " 'India must stand world .',\n",
              " 'Because I believe unless India stand world , one respect u .',\n",
              " 'Only strength respect strength .',\n",
              " 'We must strong military power also economic power .',\n",
              " 'Both must go hand-in-hand .',\n",
              " 'My good fortune worked three great mind .',\n",
              " 'Dr. Vikram Sarabhai Dept .',\n",
              " 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .',\n",
              " 'I lucky worked three closely consider great opportunity life .',\n",
              " 'I see four milestone career']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag Of Words"
      ],
      "metadata": {
        "id": "bM5Y0I5ivgi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the texts\n",
        "import re   #regular expression library for text regularization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "3b7QLIPOvee0"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "rzEk-ZU8vd4M"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph =  \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "qH08cZwHxKPb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "2nYIZJKPwc4M"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Stemming BOG"
      ],
      "metadata": {
        "id": "TJOQkHMHy2xG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])  #uses the re.sub function from the re module to replace any non-alphabetic character(numbers, punctuation,) with space\n",
        "    review = review.lower()                          #lowering helps in standardizing the text, as the BOW model typically treats words with different cases as distinct words\n",
        "    review = review.split()                          #splits the processed sentence into a list of words or tokens\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]  #refer text bolow\n",
        "    review = ' '.join(review)                        #' '.join(review) joins the processed words back into a single string with words separated by spaces\n",
        "    corpus.append(review)                            #The processed sentence is appended to the corpus list"
      ],
      "metadata": {
        "id": "iRPkzPd-wcSO"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[ps.stem(word) for word in review if not word in set(stopwords.words('english'))] is a list comprehension that iterates over the tokenized words in the review list. It removes any word that is present in the set of English stopwords using stopwords.words('english'), and then applies stemming to each remaining word using the Porter Stemmer (ps.stem(word)). This step removes common words that do not carry significant meaning and reduces words to their base form."
      ],
      "metadata": {
        "id": "UxkaWqEXKcx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tiDFTYmyw_0U",
        "outputId": "b827b30d-a4c3-41ad-dac9-c1ed80857148"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'see four mileston career'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-sNBk3zvduc",
        "outputId": "b8fdae5d-3f4b-4b62-ca65-cdee4ba6e1af"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three vision india',\n",
              " 'year histori peopl world come invad us captur land conquer mind',\n",
              " 'alexand onward greek turk mogul portugues british french dutch came loot us took',\n",
              " 'yet done nation',\n",
              " 'conquer anyon',\n",
              " 'grab land cultur histori tri enforc way life',\n",
              " '',\n",
              " 'respect freedom other first vision freedom',\n",
              " 'believ india got first vision start war independ',\n",
              " 'freedom must protect nurtur build',\n",
              " 'free one respect us',\n",
              " 'second vision india develop',\n",
              " 'fifti year develop nation',\n",
              " 'time see develop nation',\n",
              " 'among top nation world term gdp',\n",
              " 'percent growth rate area',\n",
              " 'poverti level fall',\n",
              " 'achiev global recognis today',\n",
              " 'yet lack self confid see develop nation self reliant self assur',\n",
              " 'incorrect',\n",
              " 'third vision',\n",
              " 'india must stand world',\n",
              " 'believ unless india stand world one respect us',\n",
              " 'strength respect strength',\n",
              " 'must strong militari power also econom power',\n",
              " 'must go hand hand',\n",
              " 'good fortun work three great mind',\n",
              " 'dr vikram sarabhai dept',\n",
              " 'space professor satish dhawan succeed dr brahm prakash father nuclear materi',\n",
              " 'lucki work three close consid great opportun life',\n",
              " 'see four mileston career']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer  #tool/class used to convert text data into a numerical feature matrix\n",
        "cv = CountVectorizer(max_features = 1500)   #cv instance for class CountVectorizer having max_features limit set to limit no. of features/words in BOW feature matrix to avoid high dimensionality\n",
        "X = cv.fit_transform(corpus).toarray() #see text below"
      ],
      "metadata": {
        "id": "SQY8i2vJvdiA"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Fitting the CountVectorizer:\n",
        "cv.fit_transform(corpus) is used to fit the CountVectorizer to the corpus. During this step, the CountVectorizer learns the vocabulary of the entire corpus and assigns an index to each unique word (feature) in the vocabulary.\n",
        "\n",
        "3.2. Transforming the corpus to a feature matrix:\n",
        "The fit_transform() method transforms the corpus into a numerical feature matrix. Each row in the X matrix represents a document (sentence) from the corpus, and each column represents a word from the vocabulary. The value in each cell indicates the count of how many times the corresponding word appears in the respective document.\n",
        "\n",
        "3.3. Converting the sparse matrix to an array:\n",
        "The result of fit_transform() is a sparse matrix, but the code then converts it into a dense NumPy array using .toarray(). The resulting X is now a 2-dimensional array (matrix) containing the Bag of Words representation of the corpus."
      ],
      "metadata": {
        "id": "YlgUnoXwPQYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQYlskSSx41Y",
        "outputId": "234ff9bb-ea92-42aa-f76f-b0cae0fdbb38"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 1, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-AFvMDsgZyk",
        "outputId": "e644df22-1bb6-400a-df41-d41e98bdad30"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 113)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Lemmatization BOG"
      ],
      "metadata": {
        "id": "uvb6nGr8x4ve"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "Jvx6Mi0Wx4p6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4BMyt-dZx4ij",
        "outputId": "fb395bb3-f69e-4e9f-cad2-9679d9a79bf4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'see four milestone career'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45RMHJ-Yx4Xq",
        "outputId": "58c41d7c-7634-4a3b-c1dd-387a5cf8a465"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three vision india',\n",
              " 'year history people world come invaded u captured land conquered mind',\n",
              " 'alexander onwards greek turk mogul portuguese british french dutch came looted u took',\n",
              " 'yet done nation',\n",
              " 'conquered anyone',\n",
              " 'grabbed land culture history tried enforce way life',\n",
              " '',\n",
              " 'respect freedom others first vision freedom',\n",
              " 'believe india got first vision started war independence',\n",
              " 'freedom must protect nurture build',\n",
              " 'free one respect u',\n",
              " 'second vision india development',\n",
              " 'fifty year developing nation',\n",
              " 'time see developed nation',\n",
              " 'among top nation world term gdp',\n",
              " 'percent growth rate area',\n",
              " 'poverty level falling',\n",
              " 'achievement globally recognised today',\n",
              " 'yet lack self confidence see developed nation self reliant self assured',\n",
              " 'incorrect',\n",
              " 'third vision',\n",
              " 'india must stand world',\n",
              " 'believe unless india stand world one respect u',\n",
              " 'strength respect strength',\n",
              " 'must strong military power also economic power',\n",
              " 'must go hand hand',\n",
              " 'good fortune worked three great mind',\n",
              " 'dr vikram sarabhai dept',\n",
              " 'space professor satish dhawan succeeded dr brahm prakash father nuclear material',\n",
              " 'lucky worked three closely consider great opportunity life',\n",
              " 'see four milestone career']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "6_gqQENqvdRR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR7g9c1gvcJT",
        "outputId": "b9b71da8-61fe-42c0-93cf-fd98ff21b165"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 1, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "bi-h4OjRzsV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e4c82c-f935-4c9d-b89b-111a46fa3ebf"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 114)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF"
      ],
      "metadata": {
        "id": "LdQWQ5vvfkS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#due some advantages of lemmatization over stemmer i am only using lemmatization here\n",
        "wordnet=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "QbxkrGvVzrud"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "1T_To3ivzrUk"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "VLk6v0fizrNA"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the TF-IDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "Jdmw6nTLzrGb"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X #see TF-IDF has given more importance to words but not others"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqjoHzNlgh9H",
        "outputId": "df586e15-a8c1-4a3f-9d49-cfe5097ec9ca"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.25883507, 0.30512561,\n",
              "        0.        ],\n",
              "       [0.        , 0.28867513, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRXVSvHegmvb",
        "outputId": "45da5861-907c-4857-8c2a-bccd674f9759"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 114)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec - Topic Modeling"
      ],
      "metadata": {
        "id": "5lsstG3sg-WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec #Word2Vec is class from the Gensim library used for training word embeddings\n",
        "from nltk.corpus import stopwords\n",
        "import re #regular expression"
      ],
      "metadata": {
        "id": "XT6OEPxIhHru"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code snippet demonstrates the use of the Word2Vec model from the Gensim library in Python for word embedding. It aims to train word embeddings on a corpus of text data. Word embeddings are dense vector representations of words in a continuous vector space, where semantically similar words are closer together in the vector space."
      ],
      "metadata": {
        "id": "_WdfdjfeD0iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the dataset or Preprocessing the Text Data:\n",
        "sentences = nltk.sent_tokenize(paragraph) #list of sentences\n",
        "\n",
        "words = nltk.word_tokenize(paragraph)  #list of tokens/words"
      ],
      "metadata": {
        "id": "hPwgJJVGEK4S"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Stopwords and Non-alphabetic Characters:\n",
        "corpus = [re.sub('[^a-zA-Z]', ' ', sentence).lower().split() for sentence in corpus]"
      ],
      "metadata": {
        "id": "B6xSeKGdEK2n"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyAjzTNlTSI9",
        "outputId": "61e071d2-1607-42bd-9d7e-f344946ce232"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['three', 'vision', 'india'],\n",
              " ['year',\n",
              "  'history',\n",
              "  'people',\n",
              "  'world',\n",
              "  'come',\n",
              "  'invaded',\n",
              "  'u',\n",
              "  'captured',\n",
              "  'land',\n",
              "  'conquered',\n",
              "  'mind'],\n",
              " ['alexander',\n",
              "  'onwards',\n",
              "  'greek',\n",
              "  'turk',\n",
              "  'mogul',\n",
              "  'portuguese',\n",
              "  'british',\n",
              "  'french',\n",
              "  'dutch',\n",
              "  'came',\n",
              "  'looted',\n",
              "  'u',\n",
              "  'took'],\n",
              " ['yet', 'done', 'nation'],\n",
              " ['conquered', 'anyone'],\n",
              " ['grabbed', 'land', 'culture', 'history', 'tried', 'enforce', 'way', 'life'],\n",
              " [],\n",
              " ['respect', 'freedom', 'others', 'first', 'vision', 'freedom'],\n",
              " ['believe',\n",
              "  'india',\n",
              "  'got',\n",
              "  'first',\n",
              "  'vision',\n",
              "  'started',\n",
              "  'war',\n",
              "  'independence'],\n",
              " ['freedom', 'must', 'protect', 'nurture', 'build'],\n",
              " ['free', 'one', 'respect', 'u'],\n",
              " ['second', 'vision', 'india', 'development'],\n",
              " ['fifty', 'year', 'developing', 'nation'],\n",
              " ['time', 'see', 'developed', 'nation'],\n",
              " ['among', 'top', 'nation', 'world', 'term', 'gdp'],\n",
              " ['percent', 'growth', 'rate', 'area'],\n",
              " ['poverty', 'level', 'falling'],\n",
              " ['achievement', 'globally', 'recognised', 'today'],\n",
              " ['yet',\n",
              "  'lack',\n",
              "  'self',\n",
              "  'confidence',\n",
              "  'see',\n",
              "  'developed',\n",
              "  'nation',\n",
              "  'self',\n",
              "  'reliant',\n",
              "  'self',\n",
              "  'assured'],\n",
              " ['incorrect'],\n",
              " ['third', 'vision'],\n",
              " ['india', 'must', 'stand', 'world'],\n",
              " ['believe', 'unless', 'india', 'stand', 'world', 'one', 'respect', 'u'],\n",
              " ['strength', 'respect', 'strength'],\n",
              " ['must', 'strong', 'military', 'power', 'also', 'economic', 'power'],\n",
              " ['must', 'go', 'hand', 'hand'],\n",
              " ['good', 'fortune', 'worked', 'three', 'great', 'mind'],\n",
              " ['dr', 'vikram', 'sarabhai', 'dept'],\n",
              " ['space',\n",
              "  'professor',\n",
              "  'satish',\n",
              "  'dhawan',\n",
              "  'succeeded',\n",
              "  'dr',\n",
              "  'brahm',\n",
              "  'prakash',\n",
              "  'father',\n",
              "  'nuclear',\n",
              "  'material'],\n",
              " ['lucky',\n",
              "  'worked',\n",
              "  'three',\n",
              "  'closely',\n",
              "  'consider',\n",
              "  'great',\n",
              "  'opportunity',\n",
              "  'life'],\n",
              " ['see', 'four', 'milestone', 'career']]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chat GPT\n",
        "#Training the Word2Vec Model:\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "a1N8SH9GEK0p"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp0uEuEPEKy4",
        "outputId": "8865fa42-da98-4907-ca56-3afa54837cb1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7ed6e8619240>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Word Vectors\n",
        "vector = model.wv['great']\n",
        "vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL_uzcyVOuLp",
        "outputId": "5c2f78fb-55ab-4141-9cca-99a91e513752"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8.3601912e-03, -5.6398800e-04, -9.4430409e-03,  4.7810143e-03,\n",
              "       -6.0536419e-03,  6.6430927e-03,  5.3965705e-03, -5.0263209e-03,\n",
              "        2.5755649e-03,  5.3877379e-03, -3.5955745e-03, -1.5522215e-03,\n",
              "        9.1692684e-03,  9.0911528e-03, -9.4249686e-03,  7.5686038e-03,\n",
              "        9.8924376e-03, -2.8505949e-03,  2.4508880e-03, -2.8287570e-03,\n",
              "        8.6589577e-03, -2.7359137e-04,  5.6281122e-03,  9.2254234e-03,\n",
              "        4.1028270e-03, -7.1382271e-03, -1.9021180e-03,  9.6448034e-04,\n",
              "        2.0304979e-03,  2.9639795e-03,  9.4675459e-03,  4.3962500e-03,\n",
              "        9.9427951e-03, -8.6926837e-03, -5.7867458e-03,  1.9980529e-03,\n",
              "        3.6689385e-03, -9.9819584e-04, -6.8901959e-03, -3.2532348e-03,\n",
              "       -8.5284738e-03,  9.3944669e-03,  3.7347223e-03, -7.9125548e-03,\n",
              "        3.2225966e-03,  4.1926820e-03, -5.6594731e-03, -5.9167789e-03,\n",
              "        1.0729218e-03,  8.9664767e-03, -9.6397381e-03,  9.9555546e-06,\n",
              "       -6.8464461e-03, -9.5950463e-04,  3.0334585e-03, -5.0258352e-03,\n",
              "       -2.7813141e-03,  6.8234204e-04, -6.3793738e-03,  7.2982907e-03,\n",
              "        4.3922788e-03, -8.5673518e-03, -2.1484832e-03,  3.1663883e-03,\n",
              "       -8.3372304e-03, -7.0728995e-03, -8.4581422e-03, -5.4964074e-03,\n",
              "        8.8333655e-03,  7.1098353e-03,  2.8931841e-03, -8.5614836e-03,\n",
              "        5.7638036e-03,  4.6386286e-03,  1.5162950e-04, -8.8377474e-03,\n",
              "       -1.8822077e-03,  1.9407974e-04, -7.7601336e-03,  2.4672099e-03,\n",
              "        4.7905580e-04, -7.0692012e-03, -8.3048176e-03,  6.0515166e-03,\n",
              "       -8.3493982e-03, -5.6047128e-03,  5.6086145e-03, -4.7124448e-04,\n",
              "       -3.0347023e-03, -5.1782243e-03, -1.1747357e-03,  5.2850056e-03,\n",
              "       -5.9385947e-03, -4.9714544e-03, -4.9452283e-03, -4.7577340e-03,\n",
              "       -7.9841325e-03, -9.7711524e-03,  7.6094600e-03,  7.9869526e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Most similar words\n",
        "similar = model.wv.most_similar('vikram')\n",
        "similar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrsEbnfAPTXi",
        "outputId": "1df4b875-ba5f-4e2f-9aab-c8f231cf3060"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('globally', 0.2301911860704422),\n",
              " ('consider', 0.20101553201675415),\n",
              " ('sarabhai', 0.18695951998233795),\n",
              " ('vision', 0.1832849085330963),\n",
              " ('believe', 0.1826690435409546),\n",
              " ('others', 0.15490220487117767),\n",
              " ('second', 0.15077923238277435),\n",
              " ('world', 0.14803998172283173),\n",
              " ('grabbed', 0.14361365139484406),\n",
              " ('india', 0.1367270052433014)]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the data  ###Krish Naik sir code\n",
        "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
        "text = re.sub(r'\\s+',' ',text)\n",
        "text = text.lower()\n",
        "text = re.sub(r'\\d',' ',text)\n",
        "text = re.sub(r'\\s+',' ',text)\n",
        "\n",
        "# Preparing the dataset\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
        "\n",
        "\n",
        "# Training the Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "\n",
        "#words = model.wv.vocab ###In 4.0.0 version of genkin .vocab keyword removed\n",
        "\n",
        "# Finding Word Vectors\n",
        "vector = model.wv['war']\n",
        "\n",
        "# Most similar words\n",
        "similar = model.wv.most_similar('vikram')"
      ],
      "metadata": {
        "id": "x4eikYudLy-o"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "w3Mjz_wL9X9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer #popularly used for small texts\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Input text paragraph\n",
        "text = \"I absolutely loved the movie! The acting was phenomenal, and the plot was captivating.\"\n",
        "\n",
        "# Analyze sentiment\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Determine sentiment label based on the compound score\n",
        "compound_score = sentiment_scores['compound']\n",
        "if compound_score >= 0.05:\n",
        "    sentiment_label = \"Positive\"\n",
        "elif compound_score <= -0.05:\n",
        "    sentiment_label = \"Negative\"\n",
        "else:\n",
        "    sentiment_label = \"Neutral\"\n",
        "\n",
        "# Print results\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Compound Score: {compound_score}\")\n",
        "print(f\"Sentiment Label: {sentiment_label}\")"
      ],
      "metadata": {
        "id": "G7GPMS3PspgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31984455-7579-441d-8d5d-cf813edbc04a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Results:\n",
            "Text: I absolutely loved the movie! The acting was phenomenal, and the plot was captivating.\n",
            "Compound Score: 0.6689\n",
            "Sentiment Label: Positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Input text paragraph\n",
        "text = \"I absolutely loved the movie! The acting was phenomenal, and the plot was captivating.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Determine sentiment polarity and subjectivity\n",
        "polarity = sentiment.polarity\n",
        "subjectivity = sentiment.subjectivity\n",
        "\n",
        "# Determine sentiment label based on polarity\n",
        "if polarity > 0:\n",
        "    sentiment_label = \"Positive\"\n",
        "elif polarity < 0:\n",
        "    sentiment_label = \"Negative\"\n",
        "else:\n",
        "    sentiment_label = \"Neutral\"\n",
        "\n",
        "# Print results\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Polarity: {polarity}\")\n",
        "print(f\"Subjectivity: {subjectivity}\")\n",
        "print(f\"Sentiment Label: {sentiment_label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJlclz-CBQv5",
        "outputId": "8c92e187-9884-438f-84f0-3291d98a8340"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Results:\n",
            "Text: I absolutely loved the movie! The acting was phenomenal, and the plot was captivating.\n",
            "Polarity: 0.46875\n",
            "Subjectivity: 0.575\n",
            "Sentiment Label: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis is the process of identifying and extracting the emotional tone of a text. There are different methods of sentiment analysis in NLTK, such as:\n",
        "\n",
        "1.Using NLTK’s pre-trained sentiment analyzer called VADER (Valence Aware Dictionary and sEntiment Reasoner), which uses a lexicon of words and rules to assign polarity scores to sentences.\n",
        "\n",
        "2.Customizing NLTK’s sentiment analysis by creating your own classifier using the Naive Bayes, SVM, Deep Learning algorithm and a labeled dataset of subjective texts.\n",
        "\n",
        "3.Using other NLTK modules such as TextBlob or Pattern that provide sentiment analysis functionality."
      ],
      "metadata": {
        "id": "LGI2g_lu-LWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well done... finally NLP Preprocesssing and Theory and some text level analysis done which in turn give me confidence in NLP."
      ],
      "metadata": {
        "id": "aOhv3KTUWRq7"
      }
    }
  ]
}